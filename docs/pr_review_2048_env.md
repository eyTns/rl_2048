# PR 코드리뷰: 2048 RL 환경 구현

## 리뷰 항목

### 1. PR 이름 수정
- [ ] PR 이름을 알맞게 수정하시오
- 제안: `Implement 2048 game environment for RL training`

### 2. get_valid_actions() 반환값 형식
- [x] 반환값이 0,1,2,3 중에 있는데, 이것이 머신러닝쪽에 전달하기에는 더 나은 표현인가?
- [x] StrEnum을 사용하는 것은 별로인가?
- **결론:** int가 적합함. DQN 출력과 직접 매핑 가능. 상수(ACTION_UP=0)로 가독성 확보됨.

### 3. render() 관련
- [x] render의 유닛테스트가 있는가? → **없음**
- [x] render로 나오는 결과의 예시:
```
Score: 12345
-------------------------
|  2  |  4  |  8  | 16  |
-------------------------
| 32  | 64  | 128 | 256 |
-------------------------
| 512 |1024 |2048 |     |
-------------------------
|     |     |  2  |  4  |
-------------------------
```

### 4. merge_row 테스트 케이스
- [ ] 원하는 테스트 케이스들이 있는지 확인 필요 (이따가 제시 예정)

### 5. step 함수의 reward 설계
- [x] 합쳐진 값만 reward로 주면, 합치기에만 혈안이 되어 잘못된 선택을 하는 경우를 만들지 않는가?
- [x] delayed reward 개념이 필요하지 않은가?
- [x] 이 개념은 학습하는 모듈에서 정의하면 되는 것인가?
- **결론:** 게임 환경은 기본 reward만 제공. Reward shaping, delayed reward는 학습 모듈에서 wrapper로 구현하는 것이 유연함.
