# ML 시각화 앱 설계 계획

## 모델 사양

| 항목 | 값 |
|------|-----|
| 구조 | 256 → 256 → 256 → 4 (3층 FC) |
| 입력 인코딩 | 원핫 (각 셀 × 16채널 = 4×4×16 = 256) |
| 총 파라미터 수 | 132,612개 |
| 메모리 (float64) | 약 1,036 KB |
| .npz 파일 크기 | 약 1,040 KB |

모바일 저장/전송에 부담 없는 크기입니다.

## 핵심 기능

### 1. 모델 관리
- 빈 모델 새로 생성
- 파일에서 모델 불러오기
- 학습된 모델 불러오기 (추후)
- 모델을 로컬(핸드폰, 컴퓨터)에 저장

### 2. 모델 시각화
- 모델의 파라미터를 그래프 구조로 표시
- 학습 중 가중치 변화를 선의 굵기 등으로 시각화

### 3. 실행 기능
- 현재 모델로 게임 돌리기
- TD(SARSA)로 n게임 학습하기 (기본값 n=9999)
- MC로 n게임 학습하기 (기본값 n=9999)
- 커리큘럼 모드 토글

### 4. 실시간 표시
- 매 순간 모델의 모습 (그래프)
- 매 순간 게임 판의 모습
- 학습 중 게임 운용 정보 (점수, 스텝, 보상, 선택한 행동, Q값 등)

---

## 구현설계

### 방식: 정적 HTML (서버 불필요)

모든 로직(게임, 모델, 학습, 시각화)을 JavaScript로 구현하여 단일 HTML 파일로 만듭니다.
핸드폰 브라우저에서 파일을 열기만 하면 동작합니다.

근거:
- 모델이 132,612 파라미터로 작아 JS에서 충분히 연산 가능
- `static_ui.html`에 게임 로직이 이미 JS로 구현되어 있음
- 서버 없이 모바일에서 접근 가능해야 한다는 요구사항

### 재사용할 수 있는 것

| 출처 | 내용 |
|------|------|
| `static_ui.html` | 게임 로직 (board, mergeRow, rotateBoard, move, canMove, spawnTile), 터치/키보드 입력, 타일 CSS |
| `model.py` | QNetwork 구조 (forward, backward, get_action) → JS로 포팅 |
| `trainer.py` | TD/MC 학습 로직 → JS로 포팅 |

### 파일 구조

```
ml_app.html          ← 단일 파일 (게임 + 모델 + 학습 + 시각화)
```

### 화면 구성

모바일에서는 세로로 쌓이고, 넓은 화면에서는 가로 배치됩니다.

```
┌─────────────────────────────────┐
│ 2048 RL Lab          [상태표시] │
├────────────────┬────────────────┤
│   게임판 4×4   │   정보 패널    │
│                │  · 점수        │
│                │  · 스텝/보상   │
│                │  · Q값 [4개]   │
│                │  · 행동/탐험률  │
├────────────────┴────────────────┤
│ 신경망 그래프 (Canvas)          │
│ (256) ── (256) ── (256) ── (4) │
│  입력     은닉1    은닉2   출력 │
├─────────────────────────────────┤
│ 컨트롤                         │
│ [새 모델] [불러오기] [저장]     │
│ [게임 실행] [TD n] [MC n]      │
│ [☑ 커리큘럼]                   │
└─────────────────────────────────┘
```

### 신경망 그래프 시각화 설계

256개 뉴런을 전부 개별 표시하면 연결선이 132,096개가 되어 표시 불가능합니다.

**해결: 뉴런 그룹화**

| 층 | 실제 뉴런 수 | 표시 노드 수 | 방법 |
|----|-------------|-------------|------|
| 입력 | 256 | 16 | 16개씩 묶어 16그룹 (셀당 16채널 원핫) |
| 은닉1 | 256 | 16 | 16개씩 묶어 16그룹 |
| 은닉2 | 256 | 16 | 16개씩 묶어 16그룹 |
| 출력 | 4 | 4 | 전부 표시 (↑↓←→) |

**연결선**: 16×16 + 16×16 + 16×4 = 576개 (표시 가능한 수)
- 굵기 = 그룹 간 가중치의 평균 절대값
- 색상 = 양수(파랑) / 음수(빨강)
- 학습 시 가중치가 변하면 굵기가 실시간으로 변화

### 모델 저장/불러오기 형식

브라우저에서 `.npz`를 다루기 어려우므로 JSON 형식을 사용합니다.

```json
{
  "hidden_size": 256,
  "w1": [[...], ...],  "b1": [...],
  "w2": [[...], ...],  "b2": [...],
  "w3": [[...], ...],  "b3": [...]
}
```

- 저장: JSON → Blob → `<a download>` 클릭으로 파일 다운로드
- 불러오기: `<input type="file">` → FileReader → JSON.parse

예상 JSON 크기: ~2.4MB (은닉층 256으로 float 텍스트 표현 증가)

---

## 작업 분할

### 작업 1: Game2048 JS 클래스

`static_ui.html`의 함수들을 `Game2048` 클래스로 정리합니다.
Python `game2048.py`와 동일한 인터페이스:

```
Game2048 {
    reset() → board
    step(action) → {state, reward, done}
    getValidActions() → [actions]
    getState() → board
}
```

### 작업 2: QNetwork JS 포팅

`model.py`의 QNetwork를 JavaScript로 1:1 포팅합니다.
행렬 연산은 직접 구현합니다 (외부 라이브러리 없이).
입력은 원핫 인코딩(256차원), 손실은 Huber Loss, 클리핑은 노름 기반입니다.

```
QNetwork {
    constructor(hiddenSize=256)   // Xavier 초기화
    forward(state) → qValues[4]  // 순전파 + 캐시
    backward(action, target, lr) → loss  // 역전파 + 업데이트
    getAction(state, validActions, epsilon) → action
    toJSON() → object  // 저장용
    static fromJSON(obj) → QNetwork  // 불러오기용
}
```

필요한 행렬 연산 유틸:
- `matMul(A, B)` — 행렬 곱
- `transpose(A)` — 전치
- `relu(x)`, `clip(x, min, max)` — 원소별 연산

### 작업 3: Trainer JS 포팅

`trainer.py`의 TD(SARSA)/MC 트레이너를 JavaScript로 포팅합니다.
보상 스케일링(√score), D4 대칭 증강(8배), bootstrap explosion 방지를 포함합니다.

```
TDTrainer {  // SARSA
    constructor(model, config)
    trainOneEpisode(env) → {steps, score, losses}
    // 매 스텝마다 onStep 콜백 호출
    // D4 대칭 증강: 선계산 후 학습
}

MCTrainer {
    constructor(model, config)
    trainOneEpisode(env) → {steps, score, losses}
    // 에피소드 끝에 학습, gamma 할인 적용
    // D4 대칭 증강: 선계산 후 학습
}
```

핵심: `setTimeout`/`requestAnimationFrame`으로 스텝 사이에 UI 업데이트 기회를 줍니다.
그래야 학습 중에도 게임판과 그래프가 실시간으로 갱신됩니다.

### 작업 4: 앱 레이아웃 + 게임판 UI

HTML/CSS 레이아웃과 컨트롤 패널을 만듭니다.
타일 스타일은 `static_ui.html`에서 가져옵니다.

- 게임판 렌더링
- 정보 패널 (점수, 스텝, 보상, Q값, 탐험률)
- 컨트롤 버튼 (새 모델, 불러오기, 저장, 게임 실행, TD 학습, MC 학습)
- n 입력 필드 (기본값 9999)
- 커리큘럼 모드 체크박스

### 작업 5: 신경망 그래프 시각화

Canvas에 네트워크 그래프를 그립니다.

- 4개 층의 노드 배치 (세로 열)
- 그룹화된 연결선 (576개)
- 가중치 → 선 굵기 매핑
- 양수/음수 → 색상 매핑

### 작업 6: 실시간 통합

학습/플레이 중 모든 화면을 동시에 업데이트합니다.

- 매 스텝: 게임판 갱신 + 정보 패널 갱신
- 매 스텝(또는 에피소드): 그래프 갱신 (가중치 변화 반영)
- 학습 속도 조절 (자동 vs 스텝별)

---

## 알아볼 것

- [ ] 적당한 이동방향을 선택할 전략
- [ ] 1~2수 미리읽기 전략 (모든 경우를 탐색)
- [ ] 각 수를 랜덤 시뮬레이션 여러 번 해보는 전략
- [ ] Q값은 증가하기만 하는지 확인
