<!-- 사용자 전용: AI는 이 문서를 읽되 수정하지 않음 -->

# 알아볼 것 / 할 일 / 궁금한 점

## 알아볼 것

- [ ] 학습이 어떤 계산식으로 이루어지는지 (Q-Learning 수식)
- [x] Q-Learning과 Deep Q-Learning과 RL의 차이
- [ ] Policy가 구체적으로 어떻게 구현되는지
- [x] DQN은 구체적으로 어떤 방식으로 동작하는지
- [ ] Policy와 Reward의 관계
- [ ] Q값을 정규화(normalize) 하는가?
- [ ] 머신러닝 시스템 설계 방법
- [ ] DQN의 레이어 구성
- [ ] 차원수와 모델 용량
- [ ] 회전/대칭 상태를 동일하게 처리하는 테크닉
- [ ] 학습 과정 시각화
- [ ] 2048 구현 PR 검토하기
- [ ] 게임 시각화
- [x] 게임 플레이 UI 생성
- [ ] 게임 플레이 UI PR 검토하기
- [ ] 머신러닝 장면 시각화
- [x] PR이 있을 때 추가 작업 시 브랜치 관리 방법
- [ ] Delayed Reward 개념 확인
- [ ] Discount Factor 개념 확인
- [ ] 시스템, 모듈설계
- [ ] 모듈과 레이어 정의하기
- [ ] 딥큐러닝 학습시 레이어 크기, 파라미터수 결정하기
- [ ] 적당한 이동방향을 선택할 전략
- [ ] 1~2수 미리읽기 전략 (모든 경우를 탐색)
- [ ] 각 수를 랜덤 시뮬레이션 여러 번 해보는 전략
- [ ] Q값은 증가하기만 하는지 확인
- [ ] TD 발산하는 원인 분석
- [ ] MC가 랜덤과 다를바 없는 이유 분석
- [ ] 맨처음 input 데이터로 512 같은 값 넣는지 확인
- [ ] 애초에 수가 너무 클때 ReLU, loss^2 이런것들이 망하는 이유 분석

## 궁금한 점

> reinforcement_learning.md에서 이동

- 예측할 값은 그 시점부터 최적플레이로 끝까지 갔을때 받을수있는 할인된 누적보상의 기댓값으로 정하면 되는건가?
- 감마는 턴마다 적용되는가? 감마가 너무 작지 않나? 긴건 16000턴도 갈텐데. 0.999999 해야되지않나?
- 게임 한판을 진행하면서 나온 행동들을 쭉 누적하며, 특정 턴에서 점수를 얻을때마다 이전 행동들에도 전파를 시키면서 그것들이 누적적인 학습이 일어나게 하는것인가?
- 아니면 게임 한판의 여러 턴들을 모두 저장하며 게임이 끝난 후에 학습을 쭉 시켜나가는 것인가?
- 학습의 시각화는 어떤식으로 이루어질까?
- 모델의 파라미터 색칠되는것을 볼수 있어야 하지 않을까?
- 러닝레이트 알파값은 작은게 좋은가?
- 128,128이 세로로 인접하면, 다른 더 큰수가 없어야, 가장자리로 가는게 좋은거일텐데?
- 학습시 타일들의 값 크기정보도 사용가능한 정보인데? 활용법은?
- 지금쯤 모델사이즈 정해야하나?

> ml_visualization_plan.md에서 이동

- [ ] 적당한 이동방향을 선택할 전략
- [ ] 1~2수 미리읽기 전략 (모든 경우를 탐색)
- [ ] 각 수를 랜덤 시뮬레이션 여러 번 해보는 전략
- [ ] Q값은 증가하기만 하는지 확인
