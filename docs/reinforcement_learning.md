# Reinforcement Learning Study Notes

## 기본 개념
- **State (상태)**: 현재 환경의 상황 (2048에서는 4x4 보드의 16개 숫자)
- **Action (행동)**: 에이전트가 취할 수 있는 선택 (2048에서는 상/하/좌/우)
- **Reward (보상)**: 행동에 대한 피드백, 설계가 중요함 (단순 점수보다 빈칸 수, 합쳐짐 등 고려)
- **Policy (정책)**: "이 상태에서 어떤 행동을 할까"에 대한 매핑표, RL의 목표는 최적의 Policy를 찾는 것

## 알고리즘 관계
- **RL (Reinforcement Learning)**: 강화학습 전체를 아우르는 큰 분야
- **Q-Learning**: RL의 한 알고리즘, Q-table에 (상태, 행동)별 가치를 저장
- **Deep Q-Learning (DQN)**: Q-table 대신 신경망 사용, 상태가 많을 때 필수 (2048은 상태가 무수히 많으므로 DQN 필요)

## DQN이 2048에 적용되는 구체적인 방식

### 예시 보드 상태
```
2,  4,  2,  4
4,  2,  4,  2
8,  16, 128, 64
8,  16, 128, 64
```
이 상태에서 정답은 "아래"이다 (128+128 = 256 합치기 가능).

---

### Q1. 모델이 내는 값은 무엇인가?

**Q-value (행동 가치)** 이다. "확률"이 아니다.

DQN 모델은 현재 상태 `s`를 입력받아 4개의 숫자를 출력한다:
```
Q(s, 상) = 1523.4
Q(s, 하) = 2847.2  ← 가장 높음, 이 행동 선택
Q(s, 좌) = 1102.8
Q(s, 우) = 892.1
```

**확률과의 차이점:**
- 확률: 합이 1, 각 값이 0~1 사이
- Q-value: 합이 1이 아님, 음수도 가능, 스케일 제한 없음

---

### Q2. Q-value는 정확히 무엇을 의미하는가?

**Q(s, a)의 정의:**
> "상태 s에서 행동 a를 취한 후, **최적의 정책을 따랐을 때** 게임이 끝날 때까지 받을 수 있는 **할인된 누적 보상의 기댓값**"

수식:
```
Q(s, a) = E[r₀ + γr₁ + γ²r₂ + γ³r₃ + ... | s, a, π*]
```

- `r₀`: 행동 a를 취해서 받는 즉시 보상 (예: 합쳐진 타일 값의 합)
- `γ` (감마): 할인율 (현재 구현: 0.9999)
- `π*`: 이후 최적의 행동을 취한다고 가정

**예시:**
- "아래"를 눌러 128+128=256 합치면 즉시 보상 256
- 이후 최적으로 플레이하면 추가로 ~2500점 기대
- Q(s, 하) ≈ 256 + 0.99 × 2500 ≈ 2731

**즉, "그걸 눌렀을 때 게임 끝날 때까지 더 먹을 수 있는 점수의 (할인된) 기댓값"이라고 할 수 있다.**

---

### Q3. 모델이 "아래가 정답"임을 알아내는 시점은 언제인가?

**핵심 통찰: DQN은 "미래를 예측"하는 것이지, "정답을 발견"하는 것이 아니다.**

정확히 정의된 "알아내는 시점"은 없다. 대신 점진적으로 학습된다:

#### 시나리오: 모델이 "상"을 선택했다고 가정 (탐험 중 랜덤 선택)

```
상태 s: [주어진 보드]
      ↓
모델이 "상" 선택
      ↓
즉시 보상 r = 0 (아무것도 안 합쳐짐)
      ↓
다음 상태 s': [거의 동일한 보드 + 새 타일]
      ↓
... 몇 수 후 게임 오버 (낮은 점수)
```

#### 학습이 일어나는 시점들

1. **게임 오버 시**: 총 점수가 낮았다면, 그동안의 선택들이 "나빴다"는 신호가 됨
2. **Replay 시**: Experience Replay Buffer에서 경험 (s, a, r, s')을 샘플링하여 학습할 때
3. **수천 번의 에피소드 후**: 통계적으로 "이 상태에서 아래가 좋다"라는 패턴이 Q값에 반영됨

**"정답을 알아낸다"기보다 "점점 더 정확한 기댓값을 예측하게 된다"고 표현하는 것이 정확하다.**

---

### Q4. 학습 시 어떤 값이 어떤 상태에 어떻게 반영되는가?

**SARSA (State-Action-Reward-State-Action) 학습**이 핵심이다.

현재 구현은 Q-Learning(max Q)이 아닌 SARSA(실제 다음 행동의 Q값)를 사용한다.

#### Step 1: 경험 수집
```
(s, a=하, r=256, s', a'=좌, done=False)
```
- s: 현재 상태 (주어진 보드)
- a: 선택한 행동 (아래)
- r: 즉시 보상 (256, 128+128 합침)
- s': 다음 상태 (합쳐진 후 새 타일 생성된 보드)
- a': 다음 상태에서 실제로 선택한 행동
- done: 게임 종료 여부

#### Step 2: 보상 스케일링 + 타겟 Q값 계산

보상을 100으로 나누어 스케일링한다 (Q값 × 100 ≈ 예상 실제 점수).

```
scaled_r = r / 100 = 256 / 100 = 2.56

Target = scaled_r + γ × Q(s', a')    ← SARSA: 실제 다음 행동의 Q값
       = 2.56 + 0.9999 × Q(s', 좌)
       = 2.56 + 0.9999 × 25.0  (예시)
       = 27.56
```

**Q-Learning과의 차이**: Q-Learning은 `max Q(s', a')`를 사용하지만, SARSA는 실제로 취한 `Q(s', a')`를 사용한다.

#### Step 3: D4 대칭 증강

학습 전에 현재 보드를 8가지 대칭 변환(4회전 × 2대칭)으로 증강한다.
8개의 타겟을 현재 가중치로 먼저 계산(freeze)한 후, 고정된 타겟으로 학습한다.
이를 통해 bootstrap explosion(증강 도중 가중치 변화가 타겟을 오염)을 방지한다.

#### Step 4: 손실(Loss) 계산 — Huber Loss

MSE 대신 Huber Loss (δ=1.0)를 사용하여 큰 오차에 둔감하게 처리한다.

```
현재 예측: Q(s, 하) = 22.0
Target:    27.56
error = 22.0 - 27.56 = -5.56

|error| > δ(1.0) 이므로 선형 영역:
  Loss = δ × (|error| - 0.5 × δ) = 1.0 × (5.56 - 0.5) = 5.06
  dloss = δ × sign(error) = -1.0

(|error| ≤ δ일 때는 MSE와 동일: Loss = 0.5 × error²)
```

#### Step 5: 역전파로 가중치 업데이트
```
그래디언트 노름 클리핑 (1.0) 적용 후 업데이트
Q(s, 하) ← 22.0 → 더 높은 값 방향으로 조정 (learning rate에 따라)
```

#### 일반화 효과 (DQN의 장점)

DQN은 신경망이므로:
- **비슷한 패턴을 가진 다른 보드 상태들의 Q값도 함께 조정됨**
- "128, 128이 세로로 인접해 있으면 아래가 좋다"라는 **일반화된 지식**이 형성됨
- 한 번도 본 적 없는 보드에서도 비슷한 패턴이 있으면 적절한 Q값 예측 가능

---

### 요약: DQN (SARSA) 학습의 흐름

```
[경험 수집]
    게임 플레이 → (s, a, r, s', a') 수집
         ↓
[전처리]
    보상 스케일링: r → r/100
    D4 대칭 증강: 1개 → 8개 (회전+대칭)
         ↓
[학습]
    SARSA Target = r/100 + γ × Q(s', a')
         ↓
    Huber Loss (δ=1.0) → 역전파 → 노름 클리핑 → 가중치 업데이트
         ↓
[반복]
    수천~수만 에피소드 → Q(s, a)가 실제 기댓값에 수렴
```

**핵심:** 모델은 "이 상태에서 이 행동의 기대 점수"를 학습하며, 정답/오답을 직접 알려주는 것이 아니라 경험을 통해 스스로 발견한다.

---

## 궁금한 점

- 예측할 값은 그 시점부터 최적플레이로 끝까지 갔을때 받을수있는 할인된 누적보상의 기댓값으로 정하면 되는건가?
- ~~감마는 턴마다 적용되는가? 감마가 너무 작지 않나? 긴건 16000턴도 갈텐데. 0.999999 해야되지않나?~~ → 현재 γ=0.9999 사용 중 (PR#6에서 0.999999→0.9999로 변경)
- 게임 한판을 진행하면서 나온 행동들을 쭉 누적하며, 특정 턴에서 점수를 얻을때마다 이전 행동들에도 전파를 시키면서 그것들이 누적적인 학습이 일어나게 하는것인가?
- 아니면 게임 한판의 여러 턴들을 모두 저장하며 게임이 끝난 후에 학습을 쭉 시켜나가는 것인가?
- 학습의 시각화는 어떤식으로 이루어질까?
- 모델의 파라미터 색칠되는것을 볼수 있어야 하지 않을까?
- 러닝레이트 알파값은 작은게 좋은가?
- 128,128이 세로로 인접하면, 다른 더 큰수가 없어야, 가장자리로 가는게 좋은거일텐데?
- 학습시 타일들의 값 크기정보도 사용가능한 정보인데? 활용법은?
- 지금쯤 모델사이즈 정해야하나?
